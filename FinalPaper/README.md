### Mitigating Racial Bias in Recidivism Prediction: Machine Learning & Threshold Selection Approach

### Abstract
The algorithm COMPAS, which is used by many courts in USA to assess the recidivism likelihood of defendants, has been accused of racial bias. This paper assesses COMPAS’s classification setting in defining ‘low’, ‘medium’, ‘high’ risks and tries to select proper threshold for each racial group so that the tool can improve its algorithm fairness whilst having good predictive performance. It also uses machine learning methods separately by splitting race feature as a protected attribute to examine if such method will reduce bias. 

### Research Questions:

a) Is COMPAS racially biased when scoring recidivism on defendants?

b) Following algorithm fairness definition, is current risk classifier setting for low, medium,
high risks (1-4, 5-7, 8-10) fair? If not, how should the thresholds change?

c) How might protected attributes (race/gender) separation improve the machine learning
prediction performances in ways that reduce false predictions?


### Table of contents

* [FinalPaper](https://github.com/lulululugagaga/persp-research-econ_Spr20/tree/master/FinalPaper/FinalPaper)

* [Notebook](https://github.com/lulululugagaga/persp-research-econ_Spr20/tree/master/FinalPaper/Notebook)

Code section might be helpful for reproducible study.

* [Data](https://github.com/lulululugagaga/persp-research-econ_Spr20/tree/master/FinalPaper/Data)

Data section consists of the raw data,vefication data as well as the cleaned data.

### Reference

* [COMPAS by NorthPointe](http://www.northpointeinc.com/files/downloads/FAQ_Document.pdf)
